%%----------Chapter 2------------------------------------------
\chapter{Using Yeager To Generate Long Sequence Regression Tests}
The test suite assembled in the previous chapter is a great way for a software development team to verify that the core functionality of the system under test is fundamentally operational. When executed, it will test the few well-understood scenarios we have outlined consistently and, assuming enough assertions are present, thoroughly. In fact, the suite requires the entire process from the previous chapter in order to accomodate the additon of new scenarios.

It's a boring, tedious, and repetitious task that can be the entire career of a test engineer. However, as any test automator will know, tasks which are boring, tedious, and repetitious are ripe targets for computer automation, and the task of scenario authorship is no different.

This chapter will outline a method for adapting the existing test suite explored in the previous chapter, using a tool of our own authorship named Yeager, to enable the computer to generate scenarios automatically. Yeager is an MIT-open sourced python version 3 module, with source available at \url{https://github.com/elementc/yeager}. It provides a python annotation and a set of utility functions. Usage of Yeager's state transition annotation allows testers to quickly and easily map an existing suite of test code onto a state machine, in the form of a graph. This graph can then be traversed using the utility functions, thereby generating new test scenarios from the existing code.

The resultant adapted test suite is published online at \url{https://github.com/elementc/monica-tests-yeagerized} for your convenience.

\section{Software As A State Machine}
Consider the system under test, Monica. As a relationship management web site, it has a few obvious states it can be in: logged out and on the landing page, logged in and on the dashboard, viewing a list of contacts, viewing a list of journal entries, or viewing the settings page. This maps nicely to the page objects we defined in the previous chapter. Actions on those page objects assume a current state (eg, we're logged in and on the dashboard) and after execution are in a new state which may or may not be the same state (eg, the \texttt{Dashboard.click\_contacts\_button()} method transitions from the dashboard to the contacts list, while the \texttt{LoginPage.log\_in\_incorrectly()} method should result in the system being in the same login page it was before the method was run).

In fact, most modern programs can be looked at as systems composed of a finite set of states (pages, in this case) with some state transitions (links) and a data context (the stuff you've already typed into the system in those states). Yeager uses this fact to enable automated test sequence generation.

\subsection{States in Our Example System}
Let's consider Monica's pages, which are already built into our test suite, to be states.

We have: the login page (\texttt{Login}) and logging in takes us to the \texttt{Dashboard} which has tabs for the \texttt{Contacts} list and the \texttt{Journal} log. There's also a \texttt{Settings} page which has subpages for \texttt{Import}, \texttt{Export}, \texttt{Users}, and \texttt{Tags}.

The Dashboard and Contacts list both let us \texttt{AddAContact}, while the Journal tab lets us \texttt{AddAJournalEntry}. From a given \texttt{Contact}, one can \texttt{AddASignificantOther}, \texttt{AddAChild}, \texttt{UpdateJobInformation}, \texttt{AddANote}, \texttt{AddAnActivity}, \texttt{AddAReminder}, \texttt{AddAGift}, and \texttt{AddADebt}.

For the purpose of our discussions, these pages will constitute the entire set of states in the system under test. Conveniently, each of them is a python class.

\subsection{State Transitions As Actions In Our Example System}
A graph consists of a set of nodes and a set of edges. If our nodes are the states the system under test may be in, the edges are the actions that may be taken from those states, possibly resulting in a state transition. It is certainly possible for an edge to be a loop connecting the starting state to itself. In the particular case of testing web applications, note that though it's reasonable to author a page object model with each method corresponding to an edge, this is not an assumption that is necessary to make, and would-be Yeager adopters may choose to lump lower-level page object methods into clusters of function calls in new functions and treat those higher-level functions as edges instead.

\subsection{Our Example System, Illustrated}
overview of the system as a whole, fully rendered and illustrated %TODO: FillMe

\subsection{Graph Connectedness}
"Is it possible to get from here to here?" and other questions, probably will introduce Dijkstra. %TODO: FillMe

\subsection{Capturing Contextual State}
Yeah, getting past the login screen is cool, but there's other outside influences on the output of the program than just which page we're on %TODO: FillMe

\subsection{Taking A Walk On The Graph: Long Sequence Testing}
introduce the concept of long sequence testing %TODO: FillMe

\section{Yeager State Transition Annotations}
how to use yeager: mark up your existing code %TODO: FillMe

\subsection{State Identifiers}
Anything that can be a Python dictionary key can serve as a state identifier. For simplicity's sake we use strings, but as long as Python will allow it, so will Yeager. Enterprising Yeager hackers may use the actual Python page object model class from the test suite to be adapted, often in combination with a custom random walk algorithm and the use of Python's reflection toolkit.%TODO: FillMe

\subsection{Basic State Transition Annotations}
The fastest way to get started with using Yeager is to define a function for each of the state transitions you wish to use in the test. These will probably be short snippets from the traditional-style test sequences. Then, for each of these functions, you should use the \texttt{yeager.annotations.state\_transition} annotation to mark the transition of that function. Here's an example using some of our Monica test code from the previous chapter:

{\tt
\begin{verbatim}
from pages.login import LoginPage
from pages.dashboard import DashboardPage
from yeager.annotations import state_transition

@state_transition(None, "login-page")
def open(driver):
    driver = webdriver.Chrome()
    driver.get("https://app.monicahq.com/")

@state_transition("login-page", "dashboard-page")
def log_in(driver):
    login = LoginPage(driver)
    login.log_in_correctly()

@state_transition("dashboard-page", "login-page")
def log_out(driver):
    dashboard = DashboardPage(driver)
    dashboard.log_out()
\end{verbatim}
}

Note that we use the Python \texttt{None} constant as a reference to the uninitialized system. Yeager treats \texttt{None} as a special node in the implied state model our annotations provide: it's assumed to be the entry point.

\subsection{Using The Yeager Connectedness Tester}
Yeager provides a utility function to check for misconfigured annotations. The function, \texttt{yeager.orphaned\_states}, takes one optional argument (the starting state, it defaults to \texttt{None}), and returns a list of all states that yeager knows about but doesn't know how to get to. The inverse, the known states, is also provided as a utility function with the same optional argument, as \texttt{yeager.reachable\_states}. Though the orphaned states function is useful for debugging, it can be used in other automated ways, for instance as a test coverage check or a way to automate \texttt{walk()} calls with each of the "orphaned" states actually being new entry points. These are hacker's utility tools, and can be used as such.

\section{Yeager Test Harnesses}
in more advanced scenarios, we need to assist Yeager's execution %TODO: FillMe

%\subsection{Application Configuration}
%where to put data yeager always needs %TDO: FillMe
% NOTE: THIS SECTION HAS BEEN COMmented out because yeager is presently 0-conf. It may go away soon.

\subsection{Test Setup and Entry Point}
It's up to testers to generate python scripts that start up and execute a yeager test, but the process is very easy.

The first step is to cause the python interpreter to parse all of the relevant yeager annotations. In simple test scripts, it's enough to simply write the test code and annotations at the top of the file, but in large test suites, it may be necessary to simply import those python files at the top of the yeager test script instead. Critically, yeager annotation metadata exists as long as the python interpreter instance does, so it doesn't matter what modules or other structure applies to the code the yeager annotations are spread around in. If it has been parsed, yeager knows about it.

To actually start taking a walk on the state model, simply call yeager's \texttt{walk} function with an integer representing how many steps to take. 

\subsection{Exit Point}
many scenarios won't deal with this, but how to note ways tests can end successfully. %TODO: FillMe

\subsection{Application Context Storage}
sometimes a test needs more information than just what state we're in. this overviews how to store things relevant to tests (who's expected to be logged in, how many emails they have, how many contacts, etc for an email client app) %TODO: FillMe

\subsection{Test Method Helpers}
special args to an annotation that specify a caller which pulls data from App Context Storage %TODO: FillMe

\subsection{Yeager-Only Assertions}
hooks provided for each state transition which can make additonal assertions not in the original test %TODO: FillMe

\subsection{The Yeager Logger}
Yeager doesn't make any particular assumptions about the logging toolkit that you use. It uses standard output to print log data, but it can be configured to use any arbitrary function the user supplies instead. For Long Sequence Regression Testing, it is very important to log with vigor, as a failure is often the result of many consecutive steps instead of one instant. %TODO: FillMe

\subsection{Advanced State Transition Annotations (With Context From Harness)}
using the stuff from above to enable more rich/complex state transitions %TODO: FillMe

\section{Yeager Test Plans}
the bread and butter, informing the test generator what you're wanting to do %TODO: FillMe

\subsection{Run-To-Crash vs. Run-Finitely}
discussion of a couple scenarios the tester may wish to choose between %TODO: FillMe

\subsection{Controlling The Path: Blacklists}
how to inform a test to NOT go to certain states %TODO: FillMe

\subsection{Controlling The Path: Weights}
how to inform a test to prefer (or shun) certain states %TODO: FillMe

\subsection{Controlling The Path: Visitation Limits}
how to limit the number of times a particular state should be visited (for instance, dont go to the logout state in this run, stay logged in) %TODO: FillMe

\subsection{Additional Configuration}
tbd during Yeager development %TODO: FillMe

\subsection{Executing Test Plans}
\texttt{python -m yeager run yplan.py} %TODO: FillMe

\subsection{Interpreting Results And Logs}
what do logs look like anyways? %TODO: FillMe
