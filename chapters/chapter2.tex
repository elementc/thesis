%%----------Chapter 2------------------------------------------
\chapter{Using Yeager To Generate Long Sequence Regression Tests}
The test suite assembled in the previous chapter is a great way for a software development team to verify that the core functionality of the system under test is fundamentally operational. When executed, it will test the few well-understood scenarios we have outlined consistently and, assuming enough assertions are present, thoroughly. In fact, the suite requires the entire process from the previous chapter in order to accomodate the additon of new scenarios.

It's a boring, tedious, and repetitious task that can be the entire career of a test engineer. However, as any test automator will know, tasks which are boring, tedious, and repetitious are ripe targets for computer automation, and the task of scenario authorship is no different.

This chapter will outline a method for adapting the existing test suite explored in the previous chapter, using a tool of our own authorship named Yeager, to enable the computer to generate scenarios automatically. Yeager is an MIT-open sourced python version 3 module, with source available at \url{https://github.com/elementc/yeager}. It provides a python annotation and a set of utility functions. Usage of Yeager's state transition annotation allows testers to quickly and easily map an existing suite of test code onto a state machine, in the form of a graph. This graph can then be traversed using the utility functions, thereby generating new test scenarios from the existing code.

The resultant adapted test suite is published online at \url{https://github.com/elementc/monica-tests-yeagerized} for your convenience.

\section{Software As A State Machine}
Consider the system under test, Monica. As a relationship management web site, it has a few obvious states it can be in: logged out and on the landing page, logged in and on the dashboard, viewing a list of contacts, viewing a list of journal entries, or viewing the settings page. This maps nicely to the page objects we defined in the previous chapter. Actions on those page objects assume a current state (eg, we're logged in and on the dashboard) and after execution are in a new state which may or may not be the same state (eg, the \texttt{Dashboard.click\_contacts\_button()} method transitions from the dashboard to the contacts list, while the \texttt{LoginPage.log\_in\_incorrectly()} method should result in the system being in the same login page it was before the method was run).

In fact, most modern programs can be looked at as systems composed of a finite set of states (pages, in this case) with some state transitions (links) and a data context (the stuff you've already typed into the system in those states). Yeager uses this fact to enable automated test sequence generation.

\subsection{States in Our Example System}
Let's consider Monica's pages, which are already built into our test suite, to be states.

We have: the login page (\texttt{Login}) and logging in takes us to the \texttt{Dashboard} which has tabs for the \texttt{Contacts} list and the \texttt{Journal} log. There's also a \texttt{Settings} page which has subpages for \texttt{Import}, \texttt{Export}, \texttt{Users}, and \texttt{Tags}.

The Dashboard and Contacts list both let us \texttt{AddAContact}, while the Journal tab lets us \texttt{AddAJournalEntry}. From a given \texttt{Contact}, one can \texttt{AddASignificantOther}, \texttt{AddAChild}, \texttt{UpdateJobInformation}, \texttt{AddANote}, \texttt{AddAnActivity}, \texttt{AddAReminder}, \texttt{AddAGift}, and \texttt{AddADebt}.

For the purpose of our discussions, these pages will constitute the entire set of states in the system under test. Conveniently, each of them is a python class.

\subsection{State Transitions As Actions In Our Example System}
A graph consists of a set of nodes and a set of edges. If our nodes are the states the system under test may be in, the edges are the actions that may be taken from those states, possibly resulting in a state transition. It is certainly possible for an edge to be a loop connecting the starting state to itself. In the particular case of testing web applications, note that though it's reasonable to author a page object model with each method corresponding to an edge, this is not an assumption that is necessary to make, and would-be Yeager adopters may choose to lump lower-level page object methods into clusters of function calls in new functions and treat those higher-level functions as edges instead.

\subsection{Our Example System, Illustrated}
overview of the system as a whole, fully rendered and illustrated %TODO: FillMe

\subsection{Graph Connectedness}
"Is it possible to get from here to here?" and other questions, probably will introduce Dijkstra. %TODO: FillMe

\subsection{Capturing Contextual State}
Before embarking on the journey of test automation that follows, it is important to consider for a moment the entirety of the software system. More than just software, a system includes the entire context of the software's execution, from the software itself, to the contents of the database, to the number of active system threads, to the ambient temperature of the room the system is running in. Some of these are impossible to control for in a testing environment, it's unreasonable to unseat your CPU cooler to attempt to replicate a bug related to your cousin's dust-clogged Pentium II for instance, but others are %TODO: FillMe

\subsection{Taking A Walk On The Graph}
Imagine standing on a giant picture of the program under test's state graph, at the starting point. You are able to walk along the lines in the directions they are drawn to new points but you can't walk backwards against their direction of travel. It's a strange looking environment for sure, many of the nodes you might stand on have leaving edges that just loop back to where they started. Existing test scripts are like following directions along this map, "from A, go to B, turn right at C, stop at D" and so on. These pre-planned scripts are a great way to make sure you visit the whole map at least once.

But, imagine that you had a lot of time to kill and had already done all of your maps for the day. An interesting way to spend your time might be to go wandering: wherever you are, pick one of the paths before you at random and walk that way. Keep flipping coins or rolling n-sided dice and walking and you might eventually trigger a secret passage in the labyrinth you've been wandering. Well, that or crash the program under test. How exciting!

Contrived thought experiments aside, the notion of wandering around a program is a useful one for testing. First, it simulates human usage a little more realistically than many test scenarios (how many users actually start from a freshly booted computer, load up and login to the dashboard, create one record, search for that record, delete that record, then log out?). Second, such a process can be of any arbitrary length which, while also contributing to a more realistic usage simulation (consider that the author's instance of the Atom text editor and the GNOME desktop environment has been open on their laptop since August, while the typical Atom CI instance takes 30 minutes to build the software and run all tests\citep{CircleCI}), permits test managers to use as much of the technique as they want to- wandering the program under test for a few hours on their laptop during a conference call or over a month on a virtual machine hosted in a cloud somewhere.

\section{Yeager State Transition Annotations}
how to use yeager: mark up your existing code %TODO: FillMe

\subsection{State Identifiers}
Anything that can be a Python dictionary key can serve as a state identifier. For simplicity's sake we use strings, but as long as Python will allow it, so will Yeager. Enterprising Yeager hackers may use the actual Python page object model class from the test suite to be adapted, often in combination with a custom random walk algorithm and the use of Python's reflection toolkit.%TODO: FillMe

\subsection{Basic State Transition Annotations}
The fastest way to get started with using Yeager is to define a function for each of the state transitions you wish to use in the test. These will probably be short snippets from the traditional-style test sequences. Then, for each of these functions, you should use the \texttt{yeager.annotations.state\_transition} annotation to mark the transition of that function. Here's an example using some of our Monica test code from the previous chapter:

{\tt
\begin{verbatim}
from pages.login import LoginPage
from pages.dashboard import DashboardPage
from yeager.annotations import state_transition

@state_transition(None, "login-page")
def open(driver):
    driver = webdriver.Chrome()
    driver.get("https://app.monicahq.com/")

@state_transition("login-page", "dashboard-page")
def log_in(driver):
    login = LoginPage(driver)
    login.log_in_correctly()

@state_transition("dashboard-page", "login-page")
def log_out(driver):
    dashboard = DashboardPage(driver)
    dashboard.log_out()
\end{verbatim}
}

Note that we use the Python \texttt{None} constant as a reference to the uninitialized system. Yeager treats \texttt{None} as a special node in the implied state model our annotations provide: it's assumed to be the entry point.

\subsection{Verifying Connectedness}
Yeager provides a utility function to check for states which cannot be reached from a given state, probably due to misconfigured annotations. The function, \texttt{yeager.orphaned\_states}, takes one optional argument (the starting state, it defaults to \texttt{None}), and returns a list of all states that yeager knows about but doesn't know how to get to. The inverse, the known states, is also provided as a utility function with the same optional argument, as \texttt{yeager.reachable\_states}. Though the orphaned states function is useful for debugging, it can be used in other automated ways, for instance as a test coverage check or a way to automate \texttt{walk()} calls with each of the "orphaned" states actually being new entry points. They're useful in a number of different contexts for enterprising testers.

\section{Yeager Test Harnesses}
in more advanced scenarios, we need to assist Yeager's execution %TODO: FillMe

\subsection{Test Setup and Entry Point}
It's up to testers to generate python scripts that start up and execute a yeager test, but the process is very easy.

The first step is to cause the python interpreter to parse all of the relevant yeager annotations. In simple test scripts, it's enough to simply write the test code and annotations at the top of the file, but in large test suites, it may be necessary to simply import those python files at the top of the yeager test script instead. Critically, yeager annotation metadata exists as long as the python interpreter instance does, so it doesn't matter what modules or other structure applies to the code the yeager annotations are spread around in. If it has been parsed, yeager knows about it.

To actually start taking a walk on the state model, simply call yeager's \texttt{walk} function with an integer representing how many steps to take.

% \subsection{Exit Point}
% many scenarios won't deal with this, but how to note ways tests can end successfully. %TODO: FillMe

\subsection{Application Context Storage}
sometimes a test needs more information than just what state we're in. this overviews how to store things relevant to tests (who's expected to be logged in, how many emails they have, how many contacts, etc for an email client app) %TODO: FillMe

\subsection{Test Method Helpers}
special args to an annotation that specify a caller which pulls data from App Context Storage %TODO: FillMe

\subsection{Yeager-Only Assertions}
hooks provided for each state transition which can make additonal assertions not in the original test %TODO: FillMe

\subsection{Logging in Yeager}
Yeager doesn't make any particular assumptions about the logging toolkit that you use. It uses standard output to print log data, but it can be configured to use any arbitrary function the user supplies instead. For Long Sequence Regression Testing, it is very important to log with vigor, as a failure is often the result of many consecutive steps instead of one instant. %TODO: FillMe

\subsection{Advanced State Transition Annotations (With Context From Harness)}
using the stuff from above to enable more rich/complex state transitions %TODO: FillMe

\section{Yeager Test Plans}
the bread and butter, informing the test generator what you're wanting to do %TODO: FillMe

\subsection{Run-To-Crash vs. Run-Finitely}
discussion of a couple scenarios the tester may wish to choose between %TODO: FillMe

\subsection{Controlling The Path: Blacklists}
how to inform a test to NOT go to certain states %TODO: FillMe

\subsection{Controlling The Path: Weights}
how to inform a test to prefer (or shun) certain states %TODO: FillMe

\subsection{Controlling The Path: Visitation Limits}
how to limit the number of times a particular state should be visited (for instance, dont go to the logout state in this run, stay logged in) %TODO: FillMe

\subsection{Additional Configuration}
tbd during Yeager development %TODO: FillMe

\subsection{Executing Test Plans}
\texttt{python -m yeager run yplan.py} %TODO: FillMe

\subsection{Interpreting Results And Logs}
what do logs look like anyways? %TODO: FillMe
