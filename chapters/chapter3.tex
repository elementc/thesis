%%--------------------Chapter 3------------------------
\chapter{High Volume Automated Testing And Long Sequence Regression Testing In Context}
Yeager is just one implementation of a technique called long sequence regression testing in a family of advanced testing techniques called High Volume Automated Testing, which we refer to sometimes as "High Volume Test Automation" and "HiVAT". This chapter aims to capture the wider context that Yeager exists in despite the relative novelty to academia that the field of high volume test automation provides: first through a detailed anatomy of what qualifies as such a test, second through a discussion of the craft's history as we are able to research, third through an overview of the family tree, and finally through a defense of the usefulness and reasonability of Yeager's particular branch.

\section{Anatomy Of A High Volume Automated Test}
High Volume Automated Tests are software tests that, rather than exercising a particular set of preplanned scripts as a verification of a specific requirement's compliance, algorithmically generate, execute, evaluate, and potentially summarize the results of arbitrarily many test actions on a system under test, in such volume as to attain some or all of the following goals:
\begin{enumerate}
\item Exceed the volume of a reasonable testing staff to do manually.
\item Expose behaviors of the system not normally exposed during traditional testing techniques, eg. through loading and stressing the system in a manner akin to when a large numbers of users may interact with it.
\item Simulate (ab)use of the system more realistically and dynamicaly than would be attainable through traditional techniques.
\item Generate test scenarios that, while difficult to imagine by a tester, are not outside the realm of possiblity or even probability due to the high-availability nature of modern software systems.
\end{enumerate}

While this is an extremely broad definition, this is due to the fact that these goals can be met through the arrangement of a large number of different test design factors. We propose six degrees of freedom, driven by technology choice, that combine exponentially to create a diverse and comprehensive family tree of test techniques.

\subsection{Generation: What Actions Are Taken}
The first degree of HiVAT test design freedom we propose is that of the test's generation. This is essentially an engineering question: how will the application under test be driven? In the case of yeager, tests are generated through the random selection and arrangement of pre-defined test code snippets according to a state model. Other HiVAT techniques may instead playback recorded user interactions, or send a truly random stream of input to the system, or drive the system through any other method.

\subsection{Interface: Black Box vs. White Box (And Shades Of Grey)}
A prerequisite to the above degree of test design freedom, and one that itself drives a degree, is that of the testing interface. Tests, based on the tester's ability or intent to interact with the source code or build artifacts of the system under test, may be treated as white/glass box, or black box tests. The tradeoffs associated with testing decision are well-studied, and both have valid uses.

Despite the techniques' names' implication, this is actually not a black-and-white issue: Meaningful tests can be achieved by interacting with the internals of some classes of program while still treating these interactions as black-box, dfor instance by interacting directly with the http APIs associated with a web application under test without dealing with the weight of the webapp's user interface, potentially simplifying or accelerating the test's design and execution.\citep{HoffmanTradeoffs}

\subsection{Oracle: Determining Correct Behaviour}
Purely manual testing verfies compliance through tester observation and judgement. That is, while a manual tester is working through their test script, they are looking for known indications of noncompliance, for instance bad application behavior, strange output, or system crashes. The high volume and speed of testing present in any automated test, let alone a high volume automated test, precludes human observation and therefore necessitates a computer "oracle" to verify the application under test is behaving correctly.

In the specific case of automated unit tests and related non-high-volume test techniques, testers write checkable assertions about the state of the system at various points during the test's script, which if found to be untrue are reported and treated as a test's failure. High Volume techniques may incorporate different oracles than just these assertions (though, notably, yeager doesn't). For instance, a HiVAT technique might compare the output of a system to the output generated by a previous version of the system, or a competitor's system, or a (simplistic) alternative implementation built by the tester. Or, a technique might entirely ignore the state of the system under test and might instead focus on observable metrics like system CPU load, or memory usage, or response time, or disk write patterns. The problem of "how do we know if the program is behaving correctly", also known as the "oracle problem", has many context-driven solutions, and drives this proposed degree of HiVAT design freedom.

\subsection{Loggers and Diagnostics: Figuring Out What Happened}
Diagnostics are a unique case in that, while they can act as an oracle themselves, they overlap with the fundamental question (and degree of HiVAT test design freedom) of how the tester is to determine what circumstances fed into a detected fault.

There's a broad answer, in logging, but what kind of data gets logged and what form these logs take falls to the discretion of the tester. Some tests might need detailed accountings of allocated memory bytes for every millisecond recorded, while others might get by with just a summary list- function X got called 5623 times while function Z got called 2383 times. It might be the case that a log serves as a future test plan- eg a test tool consumes its own logs to playback a previously failed test, or a tool might just dump executable test code itself.

\subsection{Testing Context: Cornering vs. Surveying vs. Abusing}
No test exists in a vacuum, even spacecraft tests for NASA. Every test sets out to prove to some stakeholder(s) that some requirement(s) are met to their satisfaction.

HiVAT techniques are incredibly versatile in that they can be used as tools to show compliance with requirements that were very difficult or expensive to verify under traditional techniques. Their adaptation can help the tester to accomplish goals like ferreting out reliable reproduction cases for intermittent "heisen"bugs, or generating leads on the breadth of bugs that might have been introduced in a new major revision, or tick the box on seemingly absurd-to-prove requirements like "the system shall not exceed a response time of 300 milliseconds more than twenty five times in thirteen million requests over the course of a one hour duration".

These testing goals, driven by the testing context, define a degree of HiVAT design freedom that exists at a higher level than the engineering and technology questions previously described.

\subsection{Scalability: Parallelized vs. Sequential}
The notion of "High Volume" does not necessitate the "high density" that is implied by the high availability of modern supercomputing, particularly inexpensive cloud systems or other massively parallel systems. That said, many systems under test are already relatively well-suited for this kind of testing, especially web API systems, and a compelling case can be made for the adoption of cloud testing techniques.\citep{parveen2010migrate} Other requirements might explictly preclude the usage of massively parallel testing techniques, for instance testing long-term use of a single-workstation native application.

This is perhaps the most limited of these six degrees of HiVAT test design freedom in that, while the decision is already made by the requirements of above design decisions, there are still (many) cases where both can apply and it becomes a design decision unto itself.

\section{A Note On The Recorded History Of High Volume Automated Testing}
It's hard to discuss the history of this family of techniques due to their origins within the field, especially as the techniques are rarely a component of a product and instead are essentially company-internal development practices- practices which companies defend jealously so as to preserve their perceived competitive advantage.

There's so little real recorded scholarship in this field, possibly driven by the fact there aren't many advanced software testing degrees offered at major universities, that occurences of rediscovery are seemingly commonplace. For instance, \citet{dawsonFourBillion} reported an interesting high volume test in which a new CPU iteration and associated software was suspected to have a bug in floating point math functions, tested when the author decided to enumerate all of the errors in the range of float, four billion or so, by iterating over each one and comparing the output of the functions under test to a known-good implementation. The author reports detecting 864,026,625 inconsistencies in the span of about ninety seconds on a computer running Microsoft Windows, which would later be used to compose the blog post reporting the test. This was a revolutionary test report among the testing community, gaining massive traction traction on HackerNews and Reddit, and as one commenter pointed out, "15 years ago this probably would not have been terribly practical. 25 years ago even thinking of it would have been absurd."

This is, of course, ignoring \citet{hoffman2003Exhausting}'s report of a test in which a new CPU iteration and associated software was suspected to have a bug in floating point math functions, tested when the author decided to enumerate all of the errors in the range of float, four billion or so, by iterating over each one and comparing the output of the functions under test to a known-good implementation. The author reports detecting 2 inconsistencies in the span of a few minutes on a parallel system about twenty times faster than a single-cpu machine of the day, which would later be used to analyze and correct images transmitted by the Hubble Space Telescope prior to OV-105 (Space Shuttle Endeavour)'s 1993 lens replacement mission, STS-61.

An absurdity, according to Reddit, but a published absurdity, which should remind us that all of this has happened before, and will happen again.

\subsection{High Volume Automated Testing Has Been Invented Six Times}
In fact, despite the fact that the first HiVAT techniques we could find in academic literature were invented one "dark and stormy night" \citep{miller1989TR830}, many anecdotal HiVAT-linkable testing regimes were in use within the software engineering industry as early as 1966 with Hewlett-Packard's unfortunately-yet-aptly named "Evil" program whose story is now lost among the millions of sufficiently dramatic modern consumer complaints about the quality of their printer drivers. In fact, \citet{KanerHivatOverview} reports several instances of HiVAT-ey testing policies, including the 1987 implementation of "ideas implenented years ago at other telephone companies", long-sequence regression testing of a word-processing application in 1984 or 1985, and a 1991 company with a popular commercial product designed to generate many fake phone-calls to enable load testing of call center systems.

In the face of Kaner's collection of anecdotal reports of HiVAT techniques used at such companies as Texas Insturments, Microsoft, AT\&T, Rolm, and other telephone companies, as well as usage in verifying FAA systems and systems at automotive manufacturers, it becomes clear that HiVAT has a rich and diverse history in industry, with many lessons to be learned from past testers. But why, then, does the academic world think the concept was invented one dark and stormy night in 1988?

\subsection{Every Industrial Inventor Thinks It's A Trade Secret}
It's the very nature of the software engineering industry that companies aim to ship products as quickly as possible and with as few bugs as they can tolerate. In fact, many managers within the industry view maintenance-related tasks (such as testing) to be a problem in need of fixing, something which is desireable to minimize, even though it is, in reality, a solution to the problems posed by external requirements, normal development faults, and human processes.\citep{glass2002Facts} It's not a crazy thought, though, as any manager would be severely concerned by the notion that roughly 80 percent of their company's development effort goes into maintenance-related tasks, and certainly seeking to cut costs in that department.\citep{pigoski1996practical}

Imagine a situation in which an enterprising tester has discovered a way to run thousands or millions of tests on a product in the span that a small team could previously do maybe a few dozen. A test manager whose first thought was to write an academic report explicating what exactly the process does and how it works so that others could apply these fascinating techniques in their own development practices would be only fired on a good day, and most likely litigated into oblivion by any rational company.

\subsection{A Call For HiVAT Documentation and Academic Consideration}
It is therefore not surprising that history became legend, legend became myth, and some testing techniques that should not have been forgotten passed out of all knowledge. There are certainly many valuable lessons still to be learned from these historical testers, and it's unlikely that this perverse incentive which drives the secreting away of advanced maintenance practices will go away in the forseeable future.

The task, then, falls to academia to find and preserve knowledge of these practices and probably many more in related niches of software engineering. The author is not aware of any professional software engineering historians, but it seems this is a rich area of further research. Perhaps, even, a career.

\section{The High Volume Test Automation Family Tree}
Despite the relative dearth of well-documented high volume automated testing prior to 1989, a large number of techniques have grown and spread since Miller's introduction of Fuzz Testing. In this section, we introduce a few well-known ones and attempt to reconcile them within our six degrees of HiVAT test design freedom.

\subsection{Long Sequence Regression Testing}
One of the simplest methods for getting started with High Volume Automated Testing is that of Long Sequence Regression Testing, wherein testers modify an existing suite of regression, integration, or functional tests such that no "cleanup" is performed and state is preserved between individual test case execution, then modifying the test runner to randomly call these test cases until the system crashes. Such modifications should only take a few hours at most assuming a sufficiently stable suite, and offer a relatively straightforward way to exploit HiVAT's ability to expose obscure bugs through simulated (ab)use of the system under test. The notion, here, is to generate scenarios that testers wouldn't think to test for but which might happen and therefore is important to identify before users get their hands on it. The kinds of bugs that this technique finds manifest as a test case that ran fine the first time, ran fine the twentieth time, but crashes the system the instant the \texttt{put\_call\_on\_hold} method gets called for the twenty first time.

In the case of LSRT, the oracle is the set of assertions built into the test suite, the generator is the modified test runner, and the testing context is surveying the system for bugs. This can be either a black box or a white box test depending on the type of suite to be modified.

Yeager shares a lot in common with this method since it too depends on existing test code assertions, acts as a replacement test runner, and is good for surveying the system for bugs. However, Yeager is advantaged versus this method since it is aware of the system's state and can consequently have more fine-grained and powerful execution than traditional LSRT implementations which treat the system as a single-state machine.

\subsection{State Model Testing}
Speaking of states, state models, also referred to as finite state machines, are useful abstractions to describe the behavior of complex systems not just in the realm of software Eegineering, but also in circuit design, communication protocols, and many other electrical and computer engineering tasks. One application of these models, known in the practice of electrical engineering as conformance testing, in which a model is treated as a specification of a system, and input sets are generated systematically and used to verify conformance to the specification by monitoring expected outputs of the system under test.\citep{lee1996principles}

The oracle, in this case, is the model and the theorems of system operation that can be generated algorithmically from it. The generator is an algorithm that consumes the model and generates input sets from it, the system under test is treated as a black box, and the testing context is attempting to survey the system under test to find behaviors of noncompliance. To be clear, this is an incredibly powerful testing procedure for this testing context, because the model is an absolute specification from which every single valid input can be theoretically derived. It is, however, prohibitively difficult to provide a fully specified model for many modern software systems, and generating one is an engineering task that vastly exceeds traditional testing tasks in terms of time and resources consumed for comparatively little gain in terms of meaningful software validation.

Yeager incorporates parts of this technique insofar as users build a state transition model of the system under test which is explored systematically (via a weighted random walk algorithm), but it's a limited implementation of a FSM as defined by Lee and Yannakakis. Yeager state transition models have no use for the Input and Output sets from their definition, and consequently disregards the $\lambda$ set as well which is comprised of the FSM's output functions. Yeager test code, however, is not necessarily ignorant of these three sets, as output checks are inherently provided by assertions built into the tester's provided state transition methods, and a global state context (for memoizing the program's Inputs and Outputs so far, for instance) can be provided by the kwargs-storing-and-forwarding feature built into Yeager.

\subsection{Exhaustive Testing}
Deferring to Dawson's and Hoffman's examples above, exhastive testing is a method of HiVAT in which a specific program function is identified for testing, and all supposedly valid inputs are fed to it so as to identify any specific values or ranges of values for which the implementation is incorrect. Usually, a known-good implementation is used as the oracle in these kinds of test, though one of Lee's FSM algorithms may also be valid.

Regardless of oracle, this method is usually only for identifying speicifc bugs in specific low-level functions, as each additional degree of input freedom (parameter) increases the testing space exponentially. Yes, a test of a function which takes one 32-bit floating point parameter is eminently exhaustable, but one that takes a set of five such parameters and requires 1.4615016e+48 unique test cases could not be completed in our lifetimes on a computer. The application of program slicing or other forms of analysis to determine whether any of these parameters are independent (reducing the increase in search space from an exponential one to an additive one) is not unreasonable and certainly a consideration of the typical "back of the napkin" calculations that might lead a tester to consider adopting this method. However, after a certain point this becomes a formal verification problem and not a testing problem.

\subsection{Fuzz and Other Monkey-Based Testing}
 Miller's Fuzz tester was a program that produced a random string of characters as input to UNIX command line utilities, purportedly inspired by a noisy dial-up phone line one dark and stormy night, monitoring the programs receivng the input for crashes or abnormal exit conditions. It's not unreasonable to imagine a truly random stream of input crashing a program eventually, and many of us fear the eventuality of a million monkeys banging on our program's metaphorical typewriters. This is especially popular among security researchers, who find the toolkit useful for identifying program vulnerabilities, though many bugs detected through other methods also may have security implications.

 Though the command line is rapidly leaving the realm of important interfaces to test relative to web and GUI interfaces, Fuzzing and its ilk have a place in the HiVAT family tree, especially as the technique inspires new tools for these new interfaces.

\subsection{Load or Perfomance Testing}
Load Based Testing is most at home in the context of web API testing. To verify an API conforms, testers will manually compose some queries that test the positive path, eg normal usage and behavior, and then maybe some queries that should be invalid or blocked or discarded so as to verify these safeguards. The test then executes each of these queries, verifies the response is as expected, and terminates. Load testing is the practice of executing arbitrarily many of these queries though the use of some sort of parallelization tool, gathering response time and other performance metrics along the way, so as to verify that the system is able to handle the load of many users simultaneously.

One such open-source tool, Locust\citep{heymanlocust}, is designed to scale to simulate millions of users and has been used extensively in industry by such companies as EA/DICE who claim the practice is "a mandatory part of the development of any large scale HTTP service built at DICE at this point." This technique is not great for ferreting out new functional bugs so much as it is at finding bugs related to massive loads. It also serves as a way to verify that system performance scales in a way consistent with operational expectations, such as server managers and bandwidth purchasers. It is inherently a black box technique.

\subsection{Testing In Production (Safely!)}
A test can be thought of as a scientific experiment in that the test's goals can be thought of as a hypothesis, for instance, "this new interation of the software doesn't break when you do this thing", and the test seeks to prove or disprove the hypothesis, usually by trying to do the thing. There's even control groups when you think of things like the previous version of the software or the requirements document, or a set of competing bugfixes or change.

In other fields, psychology for instance, humans are the determining factor in experiments. A recent trend uses unwitting human software users as the ultimate oracle in tests of things that are harder to to verify by computer. These are less integrated into the classical testing phase and more related to deployments in many cases. A Microsoft engineer on the Bing team once described to Florida Tech's school of computing seminar course their practice of staged rollouts, first by serving all users responses from the older version of the software but duplicating a portion of live queries to check the new build for crashes and to compare output of the old and new version, then by slowly transition incoming queries over to the new build and verifying user and program behavior doesn't change much, and then finally rolling the new build out to all users. At any step along the way, failure can trigger the immediate rejection of the modified candidate and reversion to the previous software iteration.

In this case, actual live user queries are the test generator, their observed behavior compared to typical behavior on the previous iteration is the oracle, and the context is verification of seamless and successful deployment of the new build. Testing In Production, previously a joke about insufficient test practices, is a valid and valuable testing technique through the lens of High Volume "Automated" Testing.

\subsection{A/B Testing}
The notion of Testing In Production has applications in other fields than just software engineering. Marketers have adapted the technique to hone their advertising campaigns, and a culture of live experimentation has prevailed in many fields. The process is simple: two or more competing yet functionally equivalent versions of, say, an advertising email, are created, and released to a subset of the audience, maybe one percent each of the whole mailing list. Test operators watch user interaction via link clicks, determine which version is performing better, then proceed to continue to distribute the winning message to the whole mailing list.

The process can be adapted to a wide number of scenarios, site homepage layouts, or app welcome screens, or search advertisements (surprise, Microsoft does this with Bing), or even which stories get presented on a news site's frontpage. It's a well-accepted practice, with companies performing millions of such experiments each year. \citep{HBR2017ABTest}

\section{The Case For Model-Based Long Sequence Regression Testing}
Compared to these established techniques, Yeager's usage is unique in that it straddles two exiting techniques, state model testing and long sequence regression testing. As discussed above, these two techniques are powerful ones when testers are hoping to survey the system for bugs. Paticularly, they finds bugs by simulating usage of the system that, in theory, should be valid, but, in practice, aren't.

There's a tradeoff between the two techniques, in that LSRT is quick to implement and leverages an existing test suite, but treats the system as a single-state machine thereby limiting the granularity of the test's execution. Contrarily, SMT necessitates the huge investment of the creation of a detailed system model, a model which might not even be feasable to implement, and has no ability to leverage an existing test automation investment. However, SMT's detailed accounting of the system's specification enables the application of systematic methods for test generation and provides fine-grained verification of the system's implementation.

Yeager is meant to be a way to bridge the gap between the two techniques. It can leverage existing test code, to accelerate implementation. It generates a relatively detailed model of the system under test enabling the application advanced model traversal algorithms (even though the current implementation only provides a weighted random walk). Yeager tests understand the the system's structure and more accurately sumulates human usage by enabling the traversal of the system in smaller, modular chunks as opposed to the same few large, repeated cases.

Yeager represents a powerful, easy-to-adopt form of HiVAT in some scenarios, enabling discovery of otherwise-elusive bugs and better testing practices across contexts.
