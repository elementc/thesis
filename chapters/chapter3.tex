%%--------------------Chapter 3------------------------
\chapter{High Volume Automated Testing And Long Sequence Regression Testing In Context}
Yeager is just one implementation of a technique called long sequence regression testing in a family of advanced testing techniques called High Volume Automated Testing, which we refer to sometimes as "High Volume Test Automation" and "HiVAT". This chapter aims to capture the wider context that Yeager exists in despite the relative novelty to academia that the field of high volume test automation provides: first through a detailed anatomy of what qualifies as such a test, second through a discussion of the craft's history as we are able to research, third through an overview of the family tree, and finally through a defense of the usefulness and reasonability of Yeager's particular branch. We conclude with a brief narrative summary of scenarios and cases in which Yeager's adoption may prove useful.

\section{Anatomy Of A High Volume Automated Test}
High Volume Automated Tests are software tests that, rather than exercising a particular set of preplanned scripts as a verification of a specific requirement's compliance, algorithmically generate, execute, evaluate, and potentially summarize the results of arbitrarily many test actions on a system under test, in such volume as to attain some or all of the following goals:
\begin{enumerate}
\item Exceed the volume of a reasonable testing staff to do manually.
\item Expose behaviors of the system not normally exposed during traditional testing techniques, eg. through loading and stressing the system in a manner akin to when a large numbers of users may interact with it.
\item Simulate (ab)use of the system more realistically and dynamicaly than would be attainable through traditional techniques.
\item Generate test scenarios that, while difficult to imagine by a tester, are not outside the realm of possiblity or even probability due to the high-availability nature of modern software systems.
\end{enumerate}

While this is an extremely broad definition, this is due to the fact that these goals can be met through the arrangement of a large number of different test design factors. We propose six degrees of freedom, driven by technology choice, that combine exponentially to create a diverse and comprehensive family tree of test techniques.

\subsection{Generation: What Actions Are Taken}
The first degree of HiVAT test design freedom we propose is that of the test's generation. This is essentially an engineering question: how will the application under test be driven? In the case of yeager, tests are generated through the random selection and arrangement of pre-defined test code snippets according to a state model. Other HiVAT techniques may instead playback recorded user interactions, or send a truly random stream of input to the system, or drive the system through any other method.

\subsection{Interface: Black Box vs. White Box (And Shades Of Grey)}
A prerequisite to the above degree of test design freedom, and one that itself drives a degree, is that of the testing interface. Tests, based on the tester's ability or intent to interact with the source code or build artifacts of the system under test, may be treated as white/glass box, or black box tests. The tradeoffs associated with testing decision are well-studied, and both have valid uses.

Despite the techniques' names' implication, this is actually not a black-and-white issue: Meaningful tests can be achieved by interacting with the internals of some classes of program while still treating these interactions as black-box, dfor instance by interacting directly with the http APIs associated with a web application under test without dealing with the weight of the webapp's user interface, potentially simplifying or accelerating the test's design and execution.\citep{HoffmanTradeoffs}

\subsection{Oracle: Determining Correct Behaviour}
Purely manual testing verfies compliance through tester observation and judgement. That is, while a manual tester is working through their test script, they are looking for known indications of noncompliance, for instance bad application behavior, strange output, or system crashes. The high volume and speed of testing present in any automated test, let alone a high volume automated test, precludes human observation and therefore necessitates a computer "oracle" to verify the application under test is behaving correctly.

In the specific case of automated unit tests and related non-high-volume test techniques, testers write checkable assertions about the state of the system at various points during the test's script, which if found to be untrue are reported and treated as a test's failure. High Volume techniques may incorporate different oracles than just these assertions (though, notably, yeager doesn't). For instance, a HiVAT technique might compare the output of a system to the output generated by a previous version of the system, or a competitor's system, or a (simplistic) alternative implementation built by the tester. Or, a technique might entirely ignore the state of the system under test and might instead focus on observable metrics like system CPU load, or memory usage, or response time, or disk write patterns. The problem of "how do we know if the program is behaving correctly", also known as the "oracle problem", has many context-driven solutions, and drives this proposed degree of HiVAT design freedom.

\subsection{Loggers and Diagnostics: Figuring Out What Happened}
Diagnostics are a unique case in that, while they can act as an oracle themselves, they overlap with the fundamental question (and degree of HiVAT test design freedom) of how the tester is to determine what circumstances fed into a detected fault.

There's a broad answer, in logging, but what kind of data gets logged and what form these logs take falls to the discretion of te tester. Some tests might need detailed accountings of allocated memory bytes for every millisecond recorded, while others might get by with just a summary list- function X got called 5623 times while function Z got called 2383 times. It might be the case that a log serves as a future test plan- eg a test tool consumes its own logs to playback a previously failed test, or a tool might just dump executable test code itself.

\subsection{Testing Context: Cornering vs. Surveying vs. Abusing}
No test exists in a vacuum, even spacecraft tests for NASA. Every test sets out to prove to some stakeholder(s) that some requirement(s) are met to their satisfaction.

HiVAT techniques are incredibly versatile in that they can be used as tools to show compliance with requirements that were very difficult or expensive to verify under traditional techniques. Their adaptation can help the tester to accomplish goals like ferreting out reliable reproduction cases for intermittent "heisen"bugs, or generating leads on the breadth of bugs that might have been introduced in a new major revision, or tick the box on seemingly absurd-to-prove requirements like "the system shall not exceed a response time of 300 milliseconds more than twenty five times in thirteen million requests over the course of a one hour duration".

These testing goals, driven by the testing context, define a degree of HiVAT design freedom that exists at a higher level than the engineering and technology questions previously described.

\subsection{Scalability: Parallelized vs. Sequential}
The notion of "High Volume" does not necessitate the "high density" that is implied by the high availability of modern supercomputing, particularly inexpensive cloud systems or other massively parallel systems. That said, many systems under test are already relatively well-suited for this kind of testing, especially web API systems, and a compelling case can be made for the adoption of cloud testing techniques.\citep{parveen2010migrate} Other requirements might explictly preclude the usage of massively parallel testing techniques, for instance testing long-term use of a single-workstation native application.

This is perhaps the most limited of these six degrees of HiVAT test design freedom in that, while the decision is already made by the requirements of above design decisions, there are still (many) cases where both can apply and it becomes a design decision unto itself.

\section{A Note On The Recorded History Of High Volume Automated Testing}
It's hard to discuss the history of this family of techniques due to their origins within the field, especially as the techniques are rarely a component of a product and instead are essentially company-internal development practices- practices which companies defend jealously so as to preserve their perceived competitive advantage.

There's so little real recorded scholarship in this field, possibly driven by the fact there aren't many advanced software testing degrees offered at major universities, that occurences of rediscovery are seemingly commonplace. For instance, \citet{dawsonFourBillion} reported an interesting high volume test in which a new CPU iteration and associated software was suspected to have a bug in floating point math functions, tested when the author decided to enumerate all of the errors in the range of float, four billion or so, by iterating over each one and comparing the output of the functions under test to a known-good implementation. The author reports detecting 864,026,625 inconsistencies in the span of about ninety seconds on a computer running Microsoft Windows, which would later be used to compose the blog post reporting the test. This was a revolutionary test report among the testing community, gaining massive traction traction on HackerNews and Reddit, and as one commenter pointed out, "15 years ago this probably would not have been terribly practical. 25 years ago even thinking of it would have been absurd."

This is, of course, ignoring \citet{hoffman2003Exhausting}'s report of a test in which a new CPU iteration and associated software was suspected to have a bug in floating point math functions, tested when the author decided to enumerate all of the errors in the range of float, four billion or so, by iterating over each one and comparing the output of the functions under test to a known-good implementation. The author reports detecting 2 inconsistencies in the span of a few minutes on a parallel system about twenty times faster than a single-cpu machine of the day, which would later be used to analyze and correct images transmitted by the Hubble Space Telescope prior to OV-105 (Space Shuttle Endeavour)'s 1993 lens replacement mission, STS-61.

An absurdity, according to Reddit, but a published absurdity, which should remind us that all of this has happened before, and will happen again.

\subsection{High Volume Automated Testing Has Been Invented Six Times}
and here's where we list all the inventors we can find.
\citep{miller1990empirical, KanerHivatOverview} %TODO: FillMe, add a citation for WTST

\subsection{Every Industrial Inventor Thinks It's A Trade Secret}
which is why I'm apologizing that this is sourced from a bunch of talks and interviews and less-than-academic sourcing. %TODO: FillMe

\subsection{A Call For HiVAT Documentation and Academic Consideration}
so that the next poor sap who writes about it isn't going to have to do so much archaeology. %TODO: FillMe

\section{The High Volume Test Automation Family Tree}
let's walk through some well-documented techniques %TODO: FillMe

\subsection{Long Sequence Regression Testing}
uh, this is the one we're talking about \citep{lee1996principles} %TODO: FillMe

\subsection{API Testing}
i'm not sure if this belongs but i've seen it on some lists %TODO: FillMe

\subsection{Exhaustive Testing}
ditto %TODO: FillMe

\subsection{"Fuzzing" And Other Monkey-Based Testing}
 "throw a fuzzer at it and see what happens" %TODO: FillMe

\subsection{Load-Based Testing}
put one of the above techniques in a thread pool of a million or so %TODO: FillMe

\subsection{Testing In Production (Safely!)}
Microsoft does this, siphons some user input from Bing to the live search engine and the next version of the search engine, comparing output from both versions. Sometimes users get output from the test version, even. %TODO: FillMe

\subsection{A/B Testing}
An aggressive version of TIP invented by marketers to compare multiple versions of the same ad campaign. %TODO: FillMe

\subsection{Synthetic HiVAT Techniques}
This is where I will wildly speculate about techniques not listed in above subsections (and therefore not discovered in literature review), but would make sense to implement in a context, as built from combinations of the building blocks listed in the Anatomy section. %TODO: FillMe

\section{High Volume Automated Testing Benefits and Drawbacks}
  this section might be merged into the above section simply due to the uniqueness of benefits and drawbacks among all the various HiVAT techniques. If, however, trends are apparent, they'll be discussed here. %TODO: FillMe

\section{The Case For Long Sequence Regression Testing}
  if there's something you could call a "conclusion", it's probably here. LSRT is a powerful, easy-to-adopt form of HiVAT in some scenarios, with otherwise-elusive bug discovery an eminently attainable outcome. %TODO: FillMe

\section{Scenarios For Yeager Adoption}
A shameless ad for different ways Yeager can be adopted by different groups (a subsection per scenario) %TODO: FillMe
