%%--------------------Chapter 3------------------------
\chapter{High Volume Automated Testing in Context}
Yeager is just one implementation of a technique called long sequence regression testing in a family of advanced testing techniques called High Volume Automated Testing, referred to sometimes as ``High Volume Test Automation'' and ``HiVAT''. This chapter captures the wider context in which Yeager exists, despite the relative novelty to academia that the field of high volume test automation provides: first through a detailed anatomy of what qualifies as such a test, second through a discussion of the craft's researchable history and third through an overview of the family tree.

\section{Anatomy of a High Volume Automated Test}
High Volume Automated Tests are software tests that, rather than exercising a particular set of preplanned scripts as a verification of a specific requirement's compliance, algorithmically generate, execute, evaluate, and potentially summarize the results of arbitrarily many test actions on a system under test, in such volume as to attain some or all of the following goals:
\begin{enumerate}
\item Exceed the volume of a reasonable testing staff to do manually.
\item Expose behaviors of the system not normally exposed during traditional testing techniques, e.g. through loading and stressing the system in a manner akin to when a large numbers of users may interact with it.
\item Simulate use and abuse of the system more realistically and dynamically than would be attainable through traditional techniques.
\item Generate test scenarios that, while difficult to imagine by a tester, are not outside the realm of possibility or even probability due to the high-availability nature of modern software systems.
\end{enumerate}

While an extremely broad definition, this is due to the fact that these goals can be met through the arrangement of a large number of different test design factors. Six vectors of freedom, many driven by technology choice, that combine exponentially to create a diverse and comprehensive family tree of test techniques, are proposed.

\subsection{Generation: What Actions Are Taken}
The first vector of HiVAT test design freedom proposed is that of the test's generation. This is essentially an engineering question: how will the application under test be driven? In the case of Yeager, tests are generated through the random selection and arrangement of pre-defined test code snippets according to the provided state model. Other HiVAT techniques may instead replay recorded user interactions, or send a random stream of input to the system, or drive the system through any other method.

\subsection{Interface: Black Box vs. White Box}
A prerequisite to the above vector of test design freedom, and one that itself drives a vector, is that of the testing interface. Tests may be treated as white/glass box, if the tester can interact with the software's build process or source code, or black box tests, if the tester only interacts with the built program. The tradeoffs associated with this testing decision are well-studied, and both have valid uses.

Despite the techniques' names' implication, this is actually not a black-and-white issue: Meaningful tests can be achieved by interacting with the internals of some classes of program while still treating these interactions as black-box, for instance by interacting directly with the http APIs associated with a web application under test without dealing with the weight of the web application's user interface, potentially simplifying or accelerating the test's design and execution.\citep{HoffmanTradeoffs}

\subsection{Oracle: Determining Correct Behavior}
Purely manual testing verifies compliance through tester observation and judgement. That is, while a manual tester is working through a test script, they are looking for known indications of noncompliance, for instance bad application behavior, strange output, or system crashes. The high volume and speed of testing present in any automated test, let alone a high volume automated test, precludes human observation and therefore necessitates a computer ``oracle'' to verify the application under test is behaving correctly.\citep{HoffmanTaxonomy}

In the specific case of automated unit tests and related non-high-volume test techniques, testers write checkable assertions about the state of the system at various points during the test's script, which if found to be untrue are reported and treated as a test's failure. High Volume techniques may incorporate different oracles than just these assertions (though, notably, yeager does not). For instance, a HiVAT technique might compare the output of a system to the output generated by a previous version of the system, or a competitor's system, or a (simplistic) alternative implementation built by the tester. Or, a technique might entirely ignore the state of the system under test and might instead focus on observable metrics like system CPU load, or memory usage, or response time, or disk write patterns. The problem summarized as ``How to know if the program is behaving correctly?'', also known as the ``oracle problem'', has many context-driven solutions, and drives this proposed vector of HiVAT design freedom.

\subsection{Loggers and Diagnostics: What Happened?}
Diagnostics are a unique case in that, while they can act as an oracle themselves, they overlap with the fundamental question (and vector of HiVAT test design freedom) of how the tester is to determine what circumstances fed into a detected fault.

There is a broad answer, in logging, but what kind of data gets logged and what form these logs take falls to the discretion of the tester. Some tests might need detailed accountings of allocated memory bytes for every millisecond recorded, while others might get by with just a summary list: ``function X was called 5623 times while function Z was called 2383 times''. It might be the case that a log serves as a future test plan, e.g. a test tool consumes its own logs to replay a previously failed test, or a tool might just dump executable test code itself.

\subsection{Testing Context: Cornering/Surveying/Abusing}
No test exists in a vacuum. Every test sets out to prove to some stakeholder(s) that some requirement(s) are met to their satisfaction.

HiVAT techniques are incredibly versatile in that they can be used as tools to show compliance with requirements that were very difficult or expensive to verify under traditional techniques. HiVAT adaptation can help the tester to accomplish goals like ferreting out reliable reproduction cases for intermittent ``Heisenbugs'', or generating leads on the breadth of bugs that might have been introduced in a new major revision, or tick the box on seemingly absurd-to-prove requirements like ``the system shall not exceed a response time of 300 milliseconds more than twenty-five times in thirteen million requests over the course of a one-hour duration''.

These testing goals, driven by the testing context, define a vector of HiVAT design freedom that exists at a higher level than the engineering and technology questions previously described.

\subsection{Scalability: Parallelized vs. Sequential}
The notion of ``high volume'' does not necessitate the ``high density'' that is implied by the high availability of modern supercomputing, particularly inexpensive cloud clusters or other massively parallel systems. Many systems under test are already relatively well-suited for this kind of testing, especially web API systems, and a compelling case can be made for the adoption of cloud testing techniques.\citep{parveen2010migrate} Other requirements might explicitly preclude the usage of massively parallel testing techniques, for instance testing long-term use of a single-workstation native application.

This is perhaps the most limited of these six vectors of HiVAT test design freedom in that, while the decision is already made by the requirements of above design decisions, there are still (many) cases where both can apply, and it becomes a design decision unto itself.

\section{On the History of High Volume Automated Testing}
It is hard to discuss the history of this family of techniques due to their origins within the field, especially as the techniques are rarely a component of a product and instead are essentially company-internal development practices- practices which companies defend jealously so as to preserve their perceived competitive advantage.

There is so little real recorded scholarship in this field, possibly driven by the fact there are not many advanced software testing degrees offered at major universities, that occurrences of rediscovery are seemingly commonplace. For instance, \citet{dawsonFourBillion} reported an interesting high volume test in which a new CPU iteration and associated software was suspected to have a bug in floating point math functions, tested when the author decided to enumerate all of the errors in the range of float (four billion or so) by iterating over each value and comparing the output of the functions under test to a known-good implementation. The author reports detecting 864,026,625 inconsistencies in the span of about ninety seconds on a computer running Microsoft Windows, which would later be used to compose the blog post reporting the test. This was a revolutionary test report among the testing community, gaining massive traction on HackerNews and Reddit, and as one commenter pointed out, ``15 years ago this probably would not have been terribly practical. 25 years ago even thinking of it would have been absurd.''

This is, of course, ignoring \citet{hoffman2003Exhausting}'s report of a test in which a new CPU iteration and associated software was suspected to have a bug in floating point math functions, tested when the author decided to enumerate all of the errors in the range of float (four billion or so) by iterating over each value and comparing the output of the functions under test to a known-good implementation. The author reports detecting 2 inconsistencies in the span of a few minutes on a parallel system about twenty times faster than a single-CPU machine of the day, which would later be used to analyze and correct images transmitted by the Hubble Space Telescope prior to OV-105 (Space Shuttle Endeavour)'s 1993 lens replacement mission, STS-61.

An absurdity, according to Reddit, but a published absurdity, which should remind us that all of this has happened before, and will happen again.

\subsection{HiVAT Has Been Invented Six Times}
In fact, despite the reality that the first HiVAT techniques found in academic literature were invented one ``dark and stormy night'' \citep{miller1989TR830}, many anecdotal HiVAT-linkable testing regimes were in use within the software engineering industry as early as 1966 with Hewlett-Packard's unfortunately-yet-aptly named ``Evil'' program whose story is now lost among the millions of sufficiently dramatic modern consumer complaints about the quality of their printer drivers. In fact, \citet{KanerHivatOverview} reports several instances of HiVAT-like testing policies, including the 1987 implementation of ``ideas implemented years ago at other telephone companies'', long-sequence regression testing of a word-processing application in 1984 or 1985, and a 1991 company with a popular commercial product designed to generate many fake phone-calls to enable load testing of call center systems.

In the face of Kaner's collection of anecdotal reports of HiVAT techniques used at such companies as Texas Instruments, Microsoft, AT\&T, Rolm, and other telephone companies, as well as usage in verifying FAA systems and systems at automotive manufacturers, it becomes clear that HiVAT has a rich and diverse history in industry, with many lessons to be learned from past testers. But why, then, does the academic world think the concept was invented one dark and stormy night in 1988?

\subsection{Every Industrial Inventor Thinks it a Trade Secret}
It is the very nature of the software engineering industry that companies aim to ship products as quickly as possible and with as few bugs as tolerable. Many managers within the industry view maintenance-related tasks (such as testing) to be a problem in need of fixing, something which is desirable to minimize, even though it is, in reality, a solution to the problems posed by external requirements, normal development faults, and human processes.\citep{glass2002Facts} Any manager would be severely concerned by the notion that roughly 80 percent of their company's development effort goes into maintenance-related tasks, and certainly seeking to cut costs in that department.\citep{pigoski1996practical}

If an enterprising tester had discovered a way to run thousands or millions of tests on a product in the span that a small team could previously run maybe a few dozen, writing an academic report explicating what exactly the process does and how it works so that others could apply these techniques in their own development practices would be result in only the tester's firing on a good day, and most likely litigation against the tester from any rational emplyer.

\subsection{A Call for Academic Consideration}
It is therefore not surprising that history became legend, legend became myth, and some testing techniques that should not have been forgotten passed out of all knowledge. There are certainly many valuable lessons still to be learned from these historical testers, and it is unlikely that this perverse incentive which drives the secreting away of advanced maintenance practices will go away in the foreseeable future.

The task, then, falls to academia to find and preserve knowledge of these practices and probably many more in related niches of software engineering. The author is not aware of any professional software engineering historians, but it seems this is a rich area of further research.

\section{The High Volume Test Automation Family Tree}
Despite the relative dearth of well-documented high volume automated testing prior to 1989, a large number of techniques have grown and spread since Millers introduction of Fuzz Testing. In this section, a few well-known ones are introduced and reconciled within some of the six suggested vectors of HiVAT test design freedom.

\subsection{Long Sequence Regression Testing}
One of the simplest methods for getting started with HiVAT is that of Long Sequence Regression Testing (LSRT), wherein testers modify an existing suite of regression, integration, or functional tests such that no ``cleanup'' is performed and state is preserved between individual test case execution, then modifying the test runner to randomly call these test cases until the system crashes. Such modifications should only take a few hours at most assuming a sufficiently stable suite, and offer a relatively straightforward way to exploit HiVAT's ability to expose obscure bugs through simulated (ab)use of the system under test. The notion is to generate scenarios that testers would not think to test for, but which might happen and therefore are important to verify before users get their hands on it. The kinds of bugs that this technique finds manifest as a test case that ran fine the first time, ran fine the twentieth time, but crashes the system the instant the \texttt{put\_call\_on\_hold} method gets called for the twenty-first time.

In the case of LSRT, the oracle is the set of assertions built into the test suite, the generator is the modified test runner, and the testing context is surveying the system for bugs. This can be either a black box or a white box test depending on the type of suite to be modified.

Yeager, too, depends on existing test code assertions, acts as a replacement test runner, and is good for surveying the system for bugs. However, Yeager is advantaged versus this method since it is aware of the system's state and can consequently have more fine-grained and powerful execution than traditional LSRT implementations which treat the system as a single-state machine.

\subsection{State Model Testing}
State models, also referred to as finite state machines, are useful abstractions to describe the behavior of complex systems not just in the realm of software engineering, but also in circuit design, communication protocols, and many other electrical and computer engineering tasks. One application of these models is known in the practice of electrical engineering as conformance testing, in which a model is treated as a specification of a system, and input sets are generated systematically and algorithmically, then used to verify conformance to the specification by monitoring expected outputs of the system under test.\citep{lee1996principles}

The oracle, in this case, is the model and the theorems of system operation that can be generated algorithmically from it. The generator is an algorithm that consumes the model and generates input sets from it, the system under test is treated as a black box, and the testing context is attempting to survey the system under test to find behaviors of noncompliance. This is an incredibly powerful testing procedure for this testing context, because the model is an absolute specification from which every single valid input can be theoretically derived. It is, however, prohibitively difficult to provide a fully specified model for many modern software systems, and generating one is an engineering task that vastly exceeds traditional testing tasks in terms of time and resources consumed for comparatively little gain in terms of meaningful software validation.

Yeager incorporates parts of this technique insofar as users build a state transition model of the system under test which is explored systematically (via a weighted random walk algorithm), but it's a limited implementation of a FSM as defined by Lee and Yannakakis. Yeager state transition models do not use the Input and Output sets from their definition, and consequently disregards the $\lambda$ set as well, which is comprised of the FSM's output functions. Yeager test code, however, is not necessarily ignorant of these three sets, as output checks are inherently provided by assertions built into the tester's provided state transition methods, and a global state context (for memoizing the program's Inputs and Outputs so far, for instance) can be provided by the kwargs-storing-and-forwarding feature built into Yeager.

\subsection{Exhaustive Testing}
Deferring to Dawson's and Hoffman's examples above, exhaustive testing is a method of HiVAT in which a specific program function is identified for testing, and all valid inputs are fed to it so as to identify any specific values or ranges of values for which the implementation is incorrect. Usually, a known-good implementation is used as the oracle in these kinds of test.

Regardless of oracle, this method is usually only for identifying specific bugs in specific low-level functions, as each additional degree of input freedom (parameter) increases the testing space exponentially. A test of a function which takes one 32-bit floating point parameter is eminently exhaustible, but one that takes a set of five such parameters and requires 1.4615016e+48 unique test cases could not be completed in a lifetime on a computer. The application of program slicing \citep{gallagher1991using} or other forms of analysis to determine whether any of these parameters are independent (reducing the increase in search space from an exponential one to an additive one) is not unreasonable and certainly a consideration of the typical ``back of the napkin'' calculations that might lead a tester to adopting this method. However, after a certain point this becomes a formal verification problem and not a testing problem.

\subsection{Fuzz and Other Monkey-Based Testing}
 Miller's Fuzz tester was a program that produced a random string of characters as input to UNIX command line utilities, purportedly inspired by a noisy dial-up phone line one dark and stormy night, monitoring the programs receiving the input for crashes or abnormal exit conditions. It is not unreasonable to imagine a random stream of input crashing a program eventually, and many of programmers fear the eventuality of a million monkeys banging on their program's metaphorical typewriters. This is especially popular among security researchers, who find the toolkit useful for identifying program vulnerabilities, though many bugs detected through other methods also may have security implications.

 Though the command line is rapidly leaving the realm of important interfaces to test relative to web and GUI interfaces, Fuzzing and related techniques have a place in the HiVAT family tree, especially as the technique inspires new tools for these new interfaces.

\subsection{Load or Performance Testing}
Load testing, sometimes referred to as performance testing is most at home in the context of web API testing. To verify an API conforms, testers will manually compose some queries that test the positive path, normal usage and behavior, and then maybe some queries that should be invalid or blocked or discarded so as to verify these safeguards. The test then executes each of these queries, verifies the response is as expected, and terminates. Load testing is the practice of executing arbitrarily many of these queries though the use of some sort of parallelization tool, gathering response time and other performance metrics along the way, so as to verify that the system is able to handle the load of many users simultaneously.

One such open-source tool, Locust\citep{heymanlocust}, is designed to scale to simulate millions of users and has been used extensively in industry by such companies as EA/DICE who claim the practice is ``a mandatory part of the development of any large scale HTTP service built at DICE at this point.'' This technique's strength lies in finding bugs related to massive loads. It also serves as a way to verify that system performance scales in a way consistent with operational expectations, such as server managers and bandwidth purchasers. It is inherently a black box technique.

\subsection{Testing in Production (Safely!)}
A test can be thought of as a scientific experiment in that the test's goals can be thought of as a hypothesis, for instance, ``this new iteration of the software does not break when this action is performed'', and the test seeks to prove or disprove the hypothesis, usually by trying to perfom that action. There are even control groups when considering things like the previous version of the software or the requirements document, or a set of competing bugfixes.

In other fields, psychology for instance, humans are the determining factor in experiments. A recent trend uses unwitting human software users as the ultimate oracle in tests of things that are harder to verify by computer. These are less integrated into the classical testing phase and more related to deployments in many cases. A Microsoft engineer once described their practice of staged rollouts, first by serving all users responses from the older version of the software but duplicating a portion of live queries to check the new build for crashes and to compare output of the old and new version, then by slowly transition incoming queries over to the new build and verifying user and program behavior does not change much, and then finally rolling the new build out to all users. At any step along the way, failure can trigger the immediate rejection of the modified candidate and reversion to the previous software iteration. \citep{AndrewsHeal}

In this case, actual live user queries are the test generator, their observed behavior compared to typical behavior on the previous iteration is the oracle, and the context is verification of seamless and successful deployment of the new build. Testing In Production, previously a joke about insufficient test practices, is a valid and valuable testing technique through the lens of High Volume ``Automated'' Testing.

\subsection{A/B Testing}
The notion of Testing In Production has applications in other fields than just software engineering. Marketers have adapted the technique to hone their advertising campaigns, and a culture of live experimentation has prevailed in many fields. The process is simple: two or more competing yet functionally equivalent versions of, say, an advertising email, are created, and released to a subset of the audience, maybe one percent each of the whole mailing list. Test operators watch user interaction via link clicks, determine which version is performing better, then proceed to continue to distribute the winning message to the whole mailing list.

The process can be adapted to a wide number of scenarios: site homepage layouts, app welcome screens, search advertisements (surprise, Microsoft does this with Bing), or even which stories get presented on a news site's frontpage. It is a well-accepted practice, with companies performing millions of such experiments each year. \citep{HBR2017ABTest}

\chapter{Conclusion: The Case for Yeager}
Yeager's usage is unique in that it straddles two exiting techniques, finite state machine (FSM) testing and long sequence regression testing. These two techniques are powerful ones when testers are hoping to survey the system for bugs. Particularly, they find bugs by simulating usage of the system that should be valid, but, in practice, is not.

There is a tradeoff between the two techniques, in that LSRT is quick to implement and leverages an existing test suite, but treats the system as a single-state machine thereby limiting the granularity and novelty of the test's execution. Contrarily, FSM testing necessitates the huge investment of the creation of a detailed system model, a model which might not even be feasible to implement, and has no ability to leverage an existing test automation investment. However, FSM testing's detailed accounting of the system's specification enables the application of systematic methods for test generation and provides fine-grained verification of the system's implementation.

Yeager is meant to be a way to bridge the gap between the two techniques. It can leverage existing test code, to accelerate implementation. It generates a relatively detailed model of the system under test enabling the application advanced model traversal algorithms (even though the current implementation only provides a weighted random walk). Yeager tests respect the system's structure and more accurately simulate human usage by enabling the traversal of the system in smaller, modular chunks as opposed to the same few large, repeated cases.

Yeager represents a powerful, easy-to-adopt form of HiVAT in some scenarios, enabling discovery of otherwise-elusive bugs and better testing practices across contexts.

\section{Contributions}
The primary contribution of this work is the creation of the Yeager open source project. The module is exercised through high volume automated tests of a web application, consuming a regression test suite purpose-built for this work as a reference implementation.

Yeager, however, is usable across contexts for generating high automated volume tests against command-line applications, API applications, desktop applications, or any codebase driveable through Python. Its clean, concise API fits on a notecard and helpfully defers to and benefits from testers' existing investment in test code- no matter what they might be testing.

Further contributions include Chapter 3's survey of the field of HiVAT, as well as testing work for the open source project Monica.

\section{Future Work}
Yeager will benefit from a number of new features in the coming months, including:
\begin{itemize}
  \item improved documentation
  \item a new termination condition for \texttt{walk} based on model coverage
  \item additional visualization sample code
\end{itemize}

The usage of the \texttt{graph\_tool} module is especially useful as the interactive window feature could enable graphical editing of the inferred graph, for instance to generate method stubs for new edges and nodes a tester could draw, or emitting simple test scripts from a tester's drawn trace of the model's graph. A breadth of new test comprehension and generation tools might be enabled by future revisions of Yeager and integration with the wider graph visualization body of work.
